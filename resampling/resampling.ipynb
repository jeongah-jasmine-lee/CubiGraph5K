{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Full Dataset Statistics: ========\n",
      "full_keep shape: (3486, 3)\n",
      "Existing full counts:\n",
      " difficulty\n",
      "medium    1958\n",
      "easy      1528\n",
      "Name: count, dtype: int64\n",
      "======== Sample Dataset Statistics: ========\n",
      "sample_keep shape: (68, 3)\n",
      "Existing sample counts:\n",
      " difficulty\n",
      "easy      34\n",
      "medium    34\n",
      "Name: count, dtype: int64\n",
      "============= After dropping hard =============\n",
      "Number of Full Dataset:  3486\n",
      "Number of Sample Dataset:  68\n",
      "\n",
      "================= Data Statistics:================\n",
      "\n",
      "Full Data Statistics:\n",
      "  Count: 3486\n",
      "  Mean: 0.3510\n",
      "  Variance: 0.0089\n",
      "\n",
      "Sample Data Statistics:\n",
      "  Count: 68\n",
      "  Mean: 0.3046\n",
      "  Variance: 0.0258\n",
      "\n",
      "Initial Cohen's d: 0.48347261079009923\n",
      "\n",
      "Need additional: 16 easy; 30 medium\n",
      "\n",
      "Selected directories to add:\n",
      "../cubicasa5k-666/high_quality_architectural/12482\n",
      "../cubicasa5k-666/high_quality_architectural/11008\n",
      "../cubicasa5k-666/high_quality_architectural/2528\n",
      "../cubicasa5k-666/high_quality/10422\n",
      "../cubicasa5k-666/high_quality_architectural/5028\n",
      "../cubicasa5k-666/high_quality/5972\n",
      "../cubicasa5k-666/high_quality_architectural/3933\n",
      "../cubicasa5k-666/high_quality_architectural/11002\n",
      "../cubicasa5k-666/high_quality_architectural/13202\n",
      "../cubicasa5k-666/high_quality_architectural/11004\n",
      "../cubicasa5k-666/high_quality_architectural/11016\n",
      "../cubicasa5k-666/high_quality_architectural/20064\n",
      "../cubicasa5k-666/high_quality/12874\n",
      "../cubicasa5k-666/high_quality_architectural/10303\n",
      "../cubicasa5k-666/high_quality/13442\n",
      "../cubicasa5k-666/high_quality_architectural/6589\n",
      "../cubicasa5k-666/high_quality_architectural/709\n",
      "../cubicasa5k-666/high_quality/10541\n",
      "../cubicasa5k-666/high_quality_architectural/12534\n",
      "../cubicasa5k-666/high_quality_architectural/7666\n",
      "../cubicasa5k-666/high_quality/7958\n",
      "../cubicasa5k-666/high_quality_architectural/98\n",
      "../cubicasa5k-666/high_quality_architectural/10614\n",
      "../cubicasa5k-666/colorful/3821\n",
      "../cubicasa5k-666/high_quality_architectural/4530\n",
      "../cubicasa5k-666/high_quality_architectural/2035\n",
      "../cubicasa5k-666/high_quality_architectural/14965\n",
      "../cubicasa5k-666/high_quality_architectural/12042\n",
      "../cubicasa5k-666/high_quality_architectural/11810\n",
      "../cubicasa5k-666/high_quality_architectural/11696\n",
      "../cubicasa5k-666/high_quality_architectural/1844\n",
      "../cubicasa5k-666/high_quality_architectural/1859\n",
      "../cubicasa5k-666/high_quality_architectural/11809\n",
      "../cubicasa5k-666/high_quality_architectural/9610\n",
      "../cubicasa5k-666/high_quality_architectural/3546\n",
      "../cubicasa5k-666/high_quality_architectural/12586\n",
      "../cubicasa5k-666/high_quality_architectural/1933\n",
      "../cubicasa5k-666/high_quality_architectural/13600\n",
      "../cubicasa5k-666/high_quality/10568\n",
      "../cubicasa5k-666/high_quality_architectural/6320\n",
      "../cubicasa5k-666/high_quality/14692\n",
      "../cubicasa5k-666/high_quality_architectural/7571\n",
      "../cubicasa5k-666/high_quality/3574\n",
      "../cubicasa5k-666/high_quality/6550\n",
      "../cubicasa5k-666/high_quality/1506\n",
      "../cubicasa5k-666/colorful/12214\n",
      "\n",
      "================= Data Statistics:================\n",
      "\n",
      "Full Data Statistics:\n",
      "  Count: 3486\n",
      "  Mean: 0.3510\n",
      "  Variance: 0.0089\n",
      "\n",
      "Sample Data Statistics:\n",
      "  Count: 114\n",
      "  Mean: 0.3203\n",
      "  Variance: 0.0157\n",
      "\n",
      "New Cohen's d: 0.3215955797870111\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_graph_info_file(txt_path):\n",
    "    records = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        current_difficulty = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Detect difficulty section headers\n",
    "            m = re.match(r'^(easy|medium|hard): \\d+ file', line)\n",
    "            if m:\n",
    "                current_difficulty = m.group(1)\n",
    "            # Detect entries with path and score\n",
    "            else:\n",
    "                m2 = re.match(r'^(.*?)\\s*\\(score:\\s*([\\d.]+)\\)', line)\n",
    "                if m2 and current_difficulty:\n",
    "                    path = m2.group(1).strip()\n",
    "                    score = float(m2.group(2))\n",
    "                    records.append({\n",
    "                        'file_path': path,\n",
    "                        'difficulty': current_difficulty,\n",
    "                        'complexity': score\n",
    "                    })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Parse the three files\n",
    "full = parse_graph_info_file('../src/graph_complexity_info_3493.txt')\n",
    "cand = parse_graph_info_file('../src/graph_complexity_info_666.txt')\n",
    "sample = parse_graph_info_file('../src/graph_complexity_info_69.txt')\n",
    "\n",
    "# 1) Retain only easy/medium from the existing sample (drop hard)\n",
    "full_keep = full[full['difficulty'].isin(['easy', 'medium'])]\n",
    "print(\"======== Full Dataset Statistics: ========\")\n",
    "print(\"full_keep shape:\", full_keep.shape)\n",
    "print(\"Existing full counts:\\n\", full_keep['difficulty'].value_counts())\n",
    "\n",
    "print(\"======== Sample Dataset Statistics: ========\")\n",
    "sample_keep = sample[sample['difficulty'].isin(['easy', 'medium'])]\n",
    "print(\"sample_keep shape:\", sample_keep.shape)\n",
    "\n",
    "# 2) Compute existing counts\n",
    "counts = sample_keep['difficulty'].value_counts()\n",
    "print(\"Existing sample counts:\\n\", counts)\n",
    "\n",
    "# 3) Cohen's d between full and existing sample\n",
    "def cohens_d(a, b):\n",
    "    nx, ny = len(a), len(b)\n",
    "    mean_diff = a.mean() - b.mean()\n",
    "    varx = a.var(ddof=1)\n",
    "    vary = b.var(ddof=1)\n",
    "    print(\"\\n================= Data Statistics:================\")\n",
    "    print(\"\\nFull Data Statistics:\")\n",
    "    print(f\"  Count: {nx}\")\n",
    "    print(f\"  Mean: {a.mean():.4f}\")\n",
    "    print(f\"  Variance: {varx:.4f}\")\n",
    "\n",
    "    print(\"\\nSample Data Statistics:\")\n",
    "    print(f\"  Count: {ny}\")\n",
    "    print(f\"  Mean: {b.mean():.4f}\")\n",
    "    print(f\"  Variance: {vary:.4f}\")\n",
    "\n",
    "    pooled_sd = np.sqrt(((nx-1)*varx + (ny-1)*vary) / (nx+ny-2))\n",
    "    return mean_diff / pooled_sd\n",
    "\n",
    "print(\"============= After dropping hard =============\")\n",
    "print(\"Number of Full Dataset: \", len(full_keep['complexity']))\n",
    "print(\"Number of Sample Dataset: \", len(sample_keep['complexity']))\n",
    "\n",
    "d_initial = cohens_d(full_keep['complexity'], sample_keep['complexity'])\n",
    "print(\"\\nInitial Cohen's d:\", d_initial)\n",
    "\n",
    "# 4) Determine how many more easy/medium are needed\n",
    "needed_easy = 50 - counts.get('easy', 0)\n",
    "needed_medium = 64 - counts.get('medium', 0)\n",
    "print(f\"\\nNeed additional: {needed_easy} easy; {needed_medium} medium\")\n",
    "\n",
    "# 5) Filter candidate pool (exclude already kept)\n",
    "cand_filtered = cand[~cand['file_path'].isin(sample_keep['file_path'])]\n",
    "cand_easy = cand_filtered[cand_filtered['difficulty']=='easy']\n",
    "cand_med = cand_filtered[cand_filtered['difficulty']=='medium']\n",
    "\n",
    "# 6) Sort candidates by closeness to full dataset mean complexity\n",
    "full_mean = full_keep['complexity'].mean()\n",
    "cand_easy = cand_easy.assign(dist=(cand_easy['complexity'] - full_mean).abs()).sort_values('dist')\n",
    "cand_med = cand_med.assign(dist=(cand_med['complexity'] - full_mean).abs()).sort_values('dist')\n",
    "\n",
    "# 7) Select the top needed from each\n",
    "selected_easy = cand_easy.head(needed_easy)\n",
    "selected_med = cand_med.head(needed_medium)\n",
    "selected = pd.concat([selected_easy, selected_med])\n",
    "\n",
    "# 8) Output the directories of the selected entries\n",
    "print(\"\\nSelected directories to add:\")\n",
    "for path in selected['file_path']:\n",
    "    print(path.rsplit('/', 1)[0])\n",
    "\n",
    "# 9) Verify new Cohen's d\n",
    "new_sample = pd.concat([sample_keep, selected])\n",
    "d_new = cohens_d(full_keep['complexity'], new_sample['complexity'])\n",
    "print(\"\\nNew Cohen's d:\", d_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Full Dataset Statistics: ========\n",
      "full_keep shape: (3486, 3)\n",
      "Existing full counts:\n",
      " difficulty\n",
      "medium    1958\n",
      "easy      1528\n",
      "Name: count, dtype: int64\n",
      "======== Sample Dataset Statistics: ========\n",
      "sample_keep shape: (68, 3)\n",
      "Existing sample counts:\n",
      " difficulty\n",
      "easy      34\n",
      "medium    34\n",
      "Name: count, dtype: int64\n",
      "============= After dropping hard =============\n",
      "Number of Full Dataset:  3486\n",
      "Number of Sample Dataset:  68\n",
      "\n",
      "Initial Cohen's d: 0.48347261079009923\n",
      "=========================================\n",
      "Need additional: 16 easy; 30 medium\n",
      "Reached |d|=0.1851 < 0.2; stopping early.\n",
      "\n",
      "Selected directories to add:\n",
      "1 :  ../cubicasa5k-666/high_quality_architectural/12482\n",
      "2 :  ../cubicasa5k-666/high_quality_architectural/11008\n",
      "3 :  ../cubicasa5k-666/high_quality_architectural/2528\n",
      "4 :  ../cubicasa5k-666/high_quality/10422\n",
      "5 :  ../cubicasa5k-666/high_quality_architectural/5028\n",
      "6 :  ../cubicasa5k-666/high_quality/5972\n",
      "7 :  ../cubicasa5k-666/high_quality_architectural/3933\n",
      "8 :  ../cubicasa5k-666/high_quality_architectural/11002\n",
      "9 :  ../cubicasa5k-666/high_quality_architectural/13202\n",
      "10 :  ../cubicasa5k-666/high_quality_architectural/11004\n",
      "11 :  ../cubicasa5k-666/high_quality_architectural/11016\n",
      "12 :  ../cubicasa5k-666/high_quality_architectural/20064\n",
      "13 :  ../cubicasa5k-666/high_quality/12874\n",
      "14 :  ../cubicasa5k-666/high_quality_architectural/10303\n",
      "15 :  ../cubicasa5k-666/high_quality/13442\n",
      "16 :  ../cubicasa5k-666/high_quality_architectural/6589\n",
      "17 :  ../cubicasa5k-666/high_quality_architectural/10151\n",
      "18 :  ../cubicasa5k-666/high_quality_architectural/10649\n",
      "19 :  ../cubicasa5k-666/high_quality_architectural/1924\n",
      "20 :  ../cubicasa5k-666/high_quality/8572\n",
      "21 :  ../cubicasa5k-666/high_quality_architectural/11650\n",
      "22 :  ../cubicasa5k-666/high_quality_architectural/12620\n",
      "23 :  ../cubicasa5k-666/high_quality_architectural/3085\n",
      "\n",
      "================= Data Statistics:================\n",
      "\n",
      "Full Data Statistics:\n",
      "  Count: 3486\n",
      "  Mean: 0.3510\n",
      "  Variance: 0.0089\n",
      "\n",
      "Sample Data Statistics:\n",
      "  Count: 91\n",
      "  Mean: 0.3331\n",
      "  Variance: 0.0265\n",
      "\n",
      "New Cohen's d: 0.1850779077798827\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_graph_info_file(txt_path):\n",
    "    records = []\n",
    "    with open(txt_path, 'r') as f:\n",
    "        current_difficulty = None\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            # Detect difficulty section headers\n",
    "            m = re.match(r'^(easy|medium|hard): \\d+ file', line)\n",
    "            if m:\n",
    "                current_difficulty = m.group(1)\n",
    "            # Detect entries with path and score\n",
    "            else:\n",
    "                m2 = re.match(r'^(.*?)\\s*\\(score:\\s*([\\d.]+)\\)', line)\n",
    "                if m2 and current_difficulty:\n",
    "                    path = m2.group(1).strip()\n",
    "                    score = float(m2.group(2))\n",
    "                    records.append({\n",
    "                        'file_path': path,\n",
    "                        'difficulty': current_difficulty,\n",
    "                        'complexity': score\n",
    "                    })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Parse the three files\n",
    "full = parse_graph_info_file('../src/graph_complexity_info_3493.txt')\n",
    "cand = parse_graph_info_file('../src/graph_complexity_info_666.txt')\n",
    "sample = parse_graph_info_file('../src/graph_complexity_info_69.txt')\n",
    "\n",
    "# 1) Retain only easy/medium from the existing sample (drop hard)\n",
    "full_keep = full[full['difficulty'].isin(['easy', 'medium'])]\n",
    "print(\"======== Full Dataset Statistics: ========\")\n",
    "print(\"full_keep shape:\", full_keep.shape)\n",
    "print(\"Existing full counts:\\n\", full_keep['difficulty'].value_counts())\n",
    "\n",
    "print(\"======== Sample Dataset Statistics: ========\")\n",
    "sample_keep = sample[sample['difficulty'].isin(['easy', 'medium'])]\n",
    "print(\"sample_keep shape:\", sample_keep.shape)\n",
    "\n",
    "# 2) Compute existing counts\n",
    "counts = sample_keep['difficulty'].value_counts()\n",
    "print(\"Existing sample counts:\\n\", counts)\n",
    "\n",
    "# 3) Cohen's d between full and existing sample\n",
    "def cohens_d(a, b):\n",
    "    nx, ny = len(a), len(b)\n",
    "    mean_diff = a.mean() - b.mean()\n",
    "    varx = a.var(ddof=1)\n",
    "    vary = b.var(ddof=1)\n",
    "\n",
    "    pooled_sd = np.sqrt(((nx-1)*varx + (ny-1)*vary) / (nx+ny-2))\n",
    "    return mean_diff / pooled_sd\n",
    "\n",
    "print(\"============= After dropping hard =============\")\n",
    "print(\"Number of Full Dataset: \", len(full_keep['complexity']))\n",
    "print(\"Number of Sample Dataset: \", len(sample_keep['complexity']))\n",
    "\n",
    "d_initial = cohens_d(full_keep['complexity'], sample_keep['complexity'])\n",
    "print(\"\\nInitial Cohen's d:\", d_initial)\n",
    "\n",
    "# 4) Determine how many more easy/medium are needed\n",
    "needed_easy = max(0, 50 - counts.get('easy', 0))\n",
    "needed_medium = max(0, 64 - counts.get('medium', 0))\n",
    "print(\"=========================================\")\n",
    "print(f\"Need additional: {needed_easy} easy; {needed_medium} medium\")\n",
    "\n",
    "# 5) Filter candidate pool (exclude already kept)\n",
    "cand_filtered = cand[~cand['file_path'].isin(sample_keep['file_path'])]\n",
    "cand_easy = cand_filtered[cand_filtered['difficulty']=='easy']\n",
    "cand_med  = cand_filtered[cand_filtered['difficulty']=='medium']\n",
    "\n",
    "# 6) Sort candidates by closeness to full dataset mean complexity\n",
    "full_mean = full_keep['complexity'].mean()\n",
    "cand_easy = cand_easy.assign(dist=(cand_easy['complexity'] - full_mean).abs())\\\n",
    "                     .sort_values('dist')\n",
    "cand_med  = cand_med.assign(dist=(cand_med['complexity']  - full_mean).abs())\\\n",
    "                    .sort_values('dist')\n",
    "\n",
    "# 7) Greedy selection to best reduce Cohen's d, stopping early if |d|<0.2\n",
    "remaining = pd.concat([cand_easy, cand_med])\n",
    "current   = sample_keep.copy()\n",
    "selected  = pd.DataFrame(columns=current.columns)\n",
    "stop_flag = False\n",
    "\n",
    "for diff, needed in [('easy', needed_easy), ('medium', needed_medium)]:\n",
    "    for _ in range(needed):\n",
    "        # compute current d and stop if already below threshold\n",
    "        curr_d = abs(cohens_d(full_keep['complexity'], current['complexity']))\n",
    "        if curr_d < 0.2:\n",
    "            print(f\"Stopping early: |d|={curr_d:.4f} < 0.2\")\n",
    "            stop_flag = True\n",
    "            break\n",
    "\n",
    "        best_idx = None\n",
    "        best_d   = curr_d\n",
    "\n",
    "        # try each remaining candidate of this difficulty\n",
    "        for idx, row in remaining[remaining['difficulty']==diff].iterrows():\n",
    "            trial = pd.concat([current, pd.DataFrame([row])], ignore_index=True)\n",
    "            d = abs(cohens_d(full_keep['complexity'], trial['complexity']))\n",
    "            if d < best_d:\n",
    "                best_d, best_idx = d, idx\n",
    "\n",
    "        if best_idx is None:\n",
    "            print(f\"No more {diff} files improve Cohen's d; stopping.\")\n",
    "            stop_flag = True\n",
    "            break\n",
    "\n",
    "        # add the best one to current & to selected\n",
    "        best_row  = remaining.loc[[best_idx]]\n",
    "        current   = pd.concat([current,  best_row], ignore_index=True)\n",
    "        selected  = pd.concat([selected, best_row], ignore_index=True)\n",
    "        remaining = remaining.drop(best_idx)\n",
    "\n",
    "        # check again after adding\n",
    "        if best_d < 0.2:\n",
    "            print(f\"Reached |d|={best_d:.4f} < 0.2; stopping early.\")\n",
    "            stop_flag = True\n",
    "            break\n",
    "\n",
    "    if stop_flag:\n",
    "        break\n",
    "\n",
    "# 8) Output the directories of the selected entries\n",
    "print(\"\\nSelected directories to add:\")\n",
    "num = 1\n",
    "for path in selected['file_path']:\n",
    "    print(num, \": \", path.rsplit('/', 1)[0])\n",
    "    num += 1\n",
    "\n",
    "# 9) Verify new Cohen's d\n",
    "new_sample = pd.concat([sample_keep, selected], ignore_index=True)\n",
    "d_new = cohens_d(full_keep['complexity'], new_sample['complexity'])\n",
    "a =full_keep['complexity']\n",
    "b = new_sample['complexity']\n",
    "nx, ny = len(a), len(b)\n",
    "mean_diff = a.mean() - b.mean()\n",
    "varx = a.var(ddof=1)\n",
    "vary = b.var(ddof=1)\n",
    "print(\"\\n================= Data Statistics:================\")\n",
    "print(\"\\nFull Data Statistics:\")\n",
    "print(f\"  Count: {nx}\")\n",
    "print(f\"  Mean: {a.mean():.4f}\")\n",
    "print(f\"  Variance: {varx:.4f}\")\n",
    "\n",
    "print(\"\\nSample Data Statistics:\")\n",
    "print(f\"  Count: {ny}\")\n",
    "print(f\"  Mean: {b.mean():.4f}\")\n",
    "print(f\"  Variance: {vary:.4f}\")\n",
    "print(\"\\nNew Cohen's d:\", d_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "floorplan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
